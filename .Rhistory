if(!exists("SCC")){
SCC <- readRDS("Source_Classification_Code.rds")
}
# as a cleaning step, convert all column headers to lower case letters without
# puncturation
colnames(NEI)<-tolower(colnames(NEI))
colnames(SCC)<-tolower(names(SCC))
colnames(SCC)<-gsub(pattern = "\\.", "", x = names(SCC))
colnames(SCC)<-gsub(pattern = "_", "", x = names(SCC))
###################################################################################
# select all indices of SCC with vehicle in the EI.Sector
matches<-grep("[Vv]ehicle", SCC$eisector)
# obtain the SCC values
sccVals<-SCC$scc[matches]
# obtain scc matches in NEI
allVehicles<-NEI[which(NEI$scc %in% sccVals),]
# subset the data to only include information for Baltimore
baltimoreVehicles<-allVehicles[allVehicles$fips=="24510",]
# melt the resulting data frame by year
meltBV<-melt(data = baltimoreVehicles,
id.vars = "year",
measure.vars = "emissions")
# cast to wide format and sum emissions
castBV<-dcast(data = meltBV,
formula = year~variable,
fun.aggregate = sum)
# create plot
png(file = "plot5.png", width = 480, height = 480, units = "px")
qplot(x = year,
y = emissions,
data = castBV ,
geom = "line",
xlab = "Year",
ylab = expression("Total Vehicular PM"[25]* " emissions (tons)"),
main = expression("Tons of Vehicular PM"[25]* " by year in Baltimore City"))
dev.off()
(## For the sake of reproducibility, the code for downloading the data sets are
## included. Scan to ####################### below for the code that generates the
## plot
## Load required packages, and read the data into the curent workspace
packages<-c("data.table", "dplyr", "reshape2", "ggplot2", "Hmisc")
sapply(packages, require, character.only = TRUE)
# function getdata() checks for the existence of a directory containing a file to
# be downloaded, and if it is not present, downloads a linked file and stores it
# in a directory in the current workspace.
#
# input: a URL linked to a file to be downloaded, desired name for the
#        directory, desired name for the downloaded file, extension for the
#        file.
# output : the path to the downloaded file
getdata<-function(fileUrl, dir, filename, ext){
# create directory, if it is not already present
dirName<-paste(dir, sep = "")
if(!file.exists(dirName)){
dir.create(path = dirName)
}
# Get the data, unless this step has already been done
dest<-paste("./", dirName,"/", filename, ext, sep = "")
if(!file.exists(dest)){
download.file(url = fileUrl,
destfile = dest,
method = "curl")
datedownloaded<-date()
}
dest
}
fileURL<-"https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
temp<-getdata(fileUrl = fileURL,
dir = "ExDa2",
filename = "ExDAData",
ext = ".zip")
epaData<-unzip(zipfile = temp)
if (!exists("NEI")){
NEI <- readRDS("summarySCC_PM25.rds")
}
if(!exists("SCC")){
SCC <- readRDS("Source_Classification_Code.rds")
}
# as a cleaning step, convert all column headers to lower case letters without
# puncturation
colnames(NEI)<-tolower(colnames(NEI))
colnames(SCC)<-tolower(names(SCC))
colnames(SCC)<-gsub(pattern = "\\.", "", x = names(SCC))
colnames(SCC)<-gsub(pattern = "_", "", x = names(SCC))
###################################################################################
# select all indices of SCC with vehicle in the EI.Sector
matches<-grep("[Vv]ehicle", SCC$eisector)
# obtain the SCC values
sccVals<-SCC$scc[matches]
# obtain scc matches in NEI
allVehicles<-NEI[which(NEI$scc %in% sccVals),]
# subset the data to only include information for Baltimore and LA
twoCityData<-allVehicles[allVehicles$fips=="24510"|allVehicles$fips=="06037",]
# melt the resulting data frame by year and city identifier
melted_twoCityData<-melt(data = twoCityData,
id.vars = c("fips", "year"),
measure.vars = "emissions")
# cast to wide format and sum emissions
cast_twoCityData<-dcast(data = melted_twoCityData,
formula = fips+year~variable,
fun.aggregate = sum)
# add a city column to the data, and identiy the city
cast_twoCityData$city<-ifelse(test = cast_twoCityData$fips=="06037",
yes = "LA",
no = "MD")
# create plot
png(file = "plot6.png", width = 480, height = 480, units = "px")
g <- ggplot(data = cast_twoCityData, aes(x = year, y = emissions, fill=city))
g <- g + geom_bar( stat="identity", binwidth=1)
g <- g + facet_grid(.~city)
g <- g + labs(x= "Year", y = expression("Total Vehicular PM"[25]* " emissions (tons)"), title = expression("Tons of Vehicular PM"[25]* " emissions by year Los Angeles and in Baltimore City"))
g
dev.off()
(## For the sake of reproducibility, the code for downloading the data sets are
## included. Scan to ####################### below for the code that generates the
## plot
## Load required packages, and read the data into the curent workspace
packages<-c("data.table", "dplyr", "reshape2", "ggplot2", "Hmisc")
sapply(packages, require, character.only = TRUE)
# function getdata() checks for the existence of a directory containing a file to
# be downloaded, and if it is not present, downloads a linked file and stores it
# in a directory in the current workspace.
#
# input: a URL linked to a file to be downloaded, desired name for the
#        directory, desired name for the downloaded file, extension for the
#        file.
# output : the path to the downloaded file
getdata<-function(fileUrl, dir, filename, ext){
# create directory, if it is not already present
dirName<-paste(dir, sep = "")
if(!file.exists(dirName)){
dir.create(path = dirName)
}
# Get the data, unless this step has already been done
dest<-paste("./", dirName,"/", filename, ext, sep = "")
if(!file.exists(dest)){
download.file(url = fileUrl,
destfile = dest,
method = "curl")
datedownloaded<-date()
}
dest
}
fileURL<-"https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
temp<-getdata(fileUrl = fileURL,
dir = "ExDa2",
filename = "ExDAData",
ext = ".zip")
epaData<-unzip(zipfile = temp)
if (!exists("NEI")){
NEI <- readRDS("summarySCC_PM25.rds")
}
if(!exists("SCC")){
SCC <- readRDS("Source_Classification_Code.rds")
}
# as a cleaning step, convert all column headers to lower case letters without
# puncturation
colnames(NEI)<-tolower(colnames(NEI))
colnames(SCC)<-tolower(names(SCC))
colnames(SCC)<-gsub(pattern = "\\.", "", x = names(SCC))
colnames(SCC)<-gsub(pattern = "_", "", x = names(SCC))
###################################################################################
# select all indices of SCC with vehicle in the EI.Sector
matches<-grep("[Vv]ehicle", SCC$eisector)
# obtain the SCC values
sccVals<-SCC$scc[matches]
# obtain scc matches in NEI
allVehicles<-NEI[which(NEI$scc %in% sccVals),]
# subset the data to only include information for Baltimore and LA
twoCityData<-allVehicles[allVehicles$fips=="24510"|allVehicles$fips=="06037",]
# melt the resulting data frame by year and city identifier
melted_twoCityData<-melt(data = twoCityData,
id.vars = c("fips", "year"),
measure.vars = "emissions")
# cast to wide format and sum emissions
cast_twoCityData<-dcast(data = melted_twoCityData,
formula = fips+year~variable,
fun.aggregate = sum)
# add a city column to the data, and identiy the city
cast_twoCityData$city<-ifelse(test = cast_twoCityData$fips=="06037",
yes = "LA",
no = "MD")
# create plot
png(file = "plot6.png", width = 500, height = 480, units = "px")
g <- ggplot(data = cast_twoCityData, aes(x = year, y = emissions, fill=city))
g <- g + geom_bar( stat="identity", binwidth=1)
g <- g + facet_grid(.~city)
g <- g + labs(x= "Year", y = expression("Total Vehicular PM"[25]* " emissions (tons)"), title = expression("Tons of Vehicular PM"[25]* " emissions by year Los Angeles and in Baltimore City"))
g
dev.off()
## For the sake of reproducibility, the code for downloading the data sets are
## included. Scan to ####################### below for the code that generates the
## plot
## Load required packages, and read the data into the curent workspace
packages<-c("data.table", "dplyr", "reshape2", "ggplot2", "Hmisc")
sapply(packages, require, character.only = TRUE)
# function getdata() checks for the existence of a directory containing a file to
# be downloaded, and if it is not present, downloads a linked file and stores it
# in a directory in the current workspace.
#
# input: a URL linked to a file to be downloaded, desired name for the
#        directory, desired name for the downloaded file, extension for the
#        file.
# output : the path to the downloaded file
getdata<-function(fileUrl, dir, filename, ext){
# create directory, if it is not already present
dirName<-paste(dir, sep = "")
if(!file.exists(dirName)){
dir.create(path = dirName)
}
# Get the data, unless this step has already been done
dest<-paste("./", dirName,"/", filename, ext, sep = "")
if(!file.exists(dest)){
download.file(url = fileUrl,
destfile = dest,
method = "curl")
datedownloaded<-date()
}
dest
}
fileURL<-"https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
temp<-getdata(fileUrl = fileURL,
dir = "ExDa2",
filename = "ExDAData",
ext = ".zip")
epaData<-unzip(zipfile = temp)
if (!exists("NEI")){
NEI <- readRDS("summarySCC_PM25.rds")
}
if(!exists("SCC")){
SCC <- readRDS("Source_Classification_Code.rds")
}
# as a cleaning step, convert all column headers to lower case letters without
# puncturation
colnames(NEI)<-tolower(colnames(NEI))
colnames(SCC)<-tolower(names(SCC))
colnames(SCC)<-gsub(pattern = "\\.", "", x = names(SCC))
colnames(SCC)<-gsub(pattern = "_", "", x = names(SCC))
###################################################################################
# select all indices of SCC with vehicle in the EI.Sector
matches<-grep("[Vv]ehicle", SCC$eisector)
# obtain the SCC values
sccVals<-SCC$scc[matches]
# obtain scc matches in NEI
allVehicles<-NEI[which(NEI$scc %in% sccVals),]
# subset the data to only include information for Baltimore and LA
twoCityData<-allVehicles[allVehicles$fips=="24510"|allVehicles$fips=="06037",]
# melt the resulting data frame by year and city identifier
melted_twoCityData<-melt(data = twoCityData,
id.vars = c("fips", "year"),
measure.vars = "emissions")
# cast to wide format and sum emissions
cast_twoCityData<-dcast(data = melted_twoCityData,
formula = fips+year~variable,
fun.aggregate = sum)
# add a city column to the data, and identiy the city
cast_twoCityData$city<-ifelse(test = cast_twoCityData$fips=="06037",
yes = "LA",
no = "MD")
# create plot
png(file = "plot6.png", width = 500, height = 480, units = "px")
g <- ggplot(data = cast_twoCityData, aes(x = year, y = emissions, fill=city))
g <- g + geom_bar( stat="identity", binwidth=1)
g <- g + facet_grid(.~city)
g <- g + labs(x= "Year",
y = expression("Total Vehicular PM"[25]* " emissions (tons)"),
title = expression("Tons of Vehicular PM"[25]* " emissions by year Los Angeles and Baltimore City"))
g
dev.off()
library(knitr)
knit("ExDA_Proj2.Rmd")
setwd("~/Desktop/GCDProject")
---
title: "GCD Project"
author: "Varun Boodram"
date: "October 8, 2014"
output: html_document
---
One of the most exciting areas in all of data science right now is wearable computing - see for example [this article](http://www.insideactivitytracking.com/data-science-activity-tracking-and-the-battle-for-the-worlds-top-sports-brand/) . Companies like Fitbit, Nike, and Jawbone Up are racing to develop the most advanced algorithms to attract new users. The data linked to from the course website represent data collected from the accelerometers from the Samsung Galaxy S smartphone. A full description is available at the site where the data was obtained: <http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones>
Create one R script called run_analysis.R that does the following:
1. Merges the training and the test sets to create one data set.
2. Extracts only the measurements on the mean and standard deviation for each measurement.
3. Uses descriptive activity names to name the activities in the data set
4. Appropriately labels the data set with descriptive variable names.
5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
### 1. Merging the data
### Load required packages
```{r, message=FALSE}
packages<-c("data.table", "dplyr", "reshape2")
sapply(packages, require, character.only = TRUE)
```
### Obtain the data programatically:
Unless this step had been performed earlier, the function getdata() was used to crete a directory to store the downloaded file, and the download performed. The linked file was a .zip file, whose contents are paths to .txt files in which the data are recorded. The enclosing directory for these files were automatically downloaded to the working directory.
```{r, cache=TRUE}
getdata<-function(fileUrl, dir, filename, ext){
# create directory, if it is not already present
dirName<-paste(dir, sep = "")
if(!file.exists(dirName)){
dir.create(path = dirName)
}
# Get the data, unless this step has already been done
dest<-paste("./", dirName,"/", filename, ext, sep = "")
if(!file.exists(dest)){
download.file(url = fileUrl, destfile = dest,
method = "curl")
datedownloaded<-date()
}
dest
}
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
temp<-getdata(fileUrl = fileURL, dir = "Fitness",
filename = "data", ext = ".zip")
alldata<-unzip(zipfile = temp)
alldata
```
### Select sets to read in for merging
The readMe was examined to determine which sets were pertinent for creating the tidy data set. Those dealing with Inertial Signals were deemed unnecessary for this project.
```{r, cache=TRUE, warning=FALSE}
library(data.table)
# grep() is used in the sequel to avoid the possibility of introducing a transcription error for alldata[i]
# read in the subject ids, and concatenate
subject_test<-fread(alldata[grep("subject_test", alldata)])
subject_train<-fread(alldata[grep("subject_train", alldata)])
subjectdata<-rbind(subject_train, subject_test)
colnames(subjectdata)<-"subjectID"
# read in the activity ids, and concatenate
activity_test<-fread(alldata[grep("(/)y_test", alldata)])
activity_train<-fread(alldata[grep("(/)y_train", alldata)])
activityids<-rbind(activity_train, activity_test)
colnames(activityids)<-"activityNum"
# read in the feature values, and concatenate
val_train<-read.table(alldata[grep("(/)X_train", alldata)])
val_test<-read.table(alldata[grep("(/)X_test", alldata)])
vals<-rbind(val_train, val_test)
# the values being measured in vals are the features. These replace the generic names
val_headers<-fread(alldata[grep("features.txt", alldata)])
colnames(val_headers)<-c("number", "feature")
# sanity check: confirm that the number of columns in vals is indeed equal to the number of rows in val_headers
c(dim(vals), dim(val_headers))
colnames(vals)<-val_headers[, feature]
# concatenate the data tables into a single data table
mergeddata<-cbind(subjectdata, activityids, vals)
```
The subject ids of merged were not concurrent. This was fixed
```{r}
unique(mergeddata$subjectID)
mergeddata<-tbl_df(arrange(mergeddata, subjectID))
unique(mergeddata$subjectID)
```
### 2. Extracting the required features
```{r}
# select from merged data the first two columns, and any others that
# contain the phrases "mean" or "std" in isolation
extracteddata<-select(mergeddata,
subjectID,
activityNum,
contains("mean\\(\\)"),
contains("std\\(\\)"))
list(Head=head(names(extracteddata), 6),
Tail=tail(names(extracteddata), 4))
```
### 3. & 4. Naming the activities in the data set
The activity names are contained in activity.labels.txt. These were merged with extracteddata by the shared variable name, activity num. The resulting data frame is tidy
```{r}
# obtain the names of each activity for each label
activity_names<-fread(alldata[grep("activity.labels", alldata)])
setnames(activity_names, c("V1", "V2"), c("activityNum", "activity"))
# remove capitalization and underscores
activity_names$activity<-tolower(activity_names$activity)
activity_names$activity<-gsub(pattern = "_",
replacement = "",
x = activity_names$activity)
# merge the data set containing the activity names with the previously computated data set
cleandata<-merge(y = extracteddata,
x = activity_names,
by = "activityNum")
cleandata<-arrange(cleandata, subjectID, activityNum)
```
### 5. Obtaining the average of each variable for each activity and each subject
cleandata was melted to long-format data with id varibales activity and subjectID. The long-formatted data was then recast to a wide format, for which the mean was caculated on each variable.
```{r, warning=FALSE}
# melt the data frame computed in the last step into a long format data frame
meltdata<-melt(cleandata, id=c("activity", "subjectID"))
# cast the long format data to wide format, and call mean() on all variables but subjectID and activity
averaged_data<-dcast(data = meltdata,
formula = subjectID+activity~variable,
fun.aggregate = mean)
#arrange the data by both subjectID and activityNum
averaged_data<-arrange(averaged_data, subjectID, activityNum)
# the column names of averaged_data need to be changed to reflect the fact that the data they contain are averaged values
new_colNames<-paste("ave", colnames(averaged_data), sep=".")
setnames(x = averaged_data,
old = colnames(averaged_data[4:ncol(averaged_data)]),
new = new_colNames[4:length(new_colNames)])
write.table(x = averaged_data,
file = "./Fitness/averagedData.txt",
row.names = F)
```
If the resulting table is examined with
```{r,results='hide'}
data<-read.table(file = "./Fitness/averagedData.txt", header = T)
View(data)
```
* each observation (subject) is in itss own row
* each variable (activity:ave.fBodyBodyGyroJerkMag-std()) is in its own column
* there is a single observation type, and so, a single table
getwd()
## Step 1: merge the data into a single dataset
# function getdata() checks for the existence of a directory containing a file to
# be downloaded, and if it is not present, downloads a linked file and stores it
# in a directory in the current workspace.
#
# input: a URL linked to a file to be downloaded, desired name for the
#        directory, desired name for the downloaded file, extension for the
#        file.
# output : the path to the downloaded file
getdata<-function(fileUrl, dir, filename, ext){
# create directory, if it is not already present
dirName<-paste(dir, sep = "")
if(!file.exists(dirName)){
dir.create(path = dirName)
}
# Get the data, unless this step has already been done
dest<-paste("./", dirName,"/", filename, ext, sep = "")
if(!file.exists(dest)){
download.file(url = fileUrl, destfile = dest,
method = "curl")
datedownloaded<-date()
}
dest
}
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fprojectfiles%2FUCI%20HAR%20Dataset.zip"
temp<-getdata(fileUrl = fileURL, dir = "Fitness",
filename = "data", ext = ".zip")
alldata<-unzip(zipfile = temp)
# grep() is used in the sequel to avoid the possibility of introducing a transcription error for alldata[i]
# read in the subject ids, and concatenate
subject_test<-fread(alldata[grep("subject_test", alldata)])
subject_train<-fread(alldata[grep("subject_train", alldata)])
subjectdata<-rbind(subject_train, subject_test)
colnames(subjectdata)<-"subjectID"
# read in the activity ids, and concatenate
activity_test<-fread(alldata[grep("(/)y_test", alldata)])
activity_train<-fread(alldata[grep("(/)y_train", alldata)])
activityids<-rbind(activity_train, activity_test)
colnames(activityids)<-"activityNum"
# read in the feature values, and concatenate
val_train<-read.table(alldata[grep("(/)X_train", alldata)])
val_test<-read.table(alldata[grep("(/)X_test", alldata)])
vals<-rbind(val_train, val_test)
# the values being measured in vals are the features. These replace the generic
# names
val_headers<-fread(alldata[grep("features.txt", alldata)])
colnames(val_headers)<-c("number", "feature")
# sanity check: confirm that the number of columns in vals is indeed equal to the
# number of rows in val_headers
c(dim(vals), dim(val_headers))
colnames(vals)<-val_headers[, feature]
# concatenate the data tables into a single data table
mergeddata<-cbind(subjectdata, activityids, vals)
## Extract only the measurements on the mean and standard deviation for
## each measurement and label the data set with descriptive variable names
# select from merged data the first two columns, and any others that
# contain the phrases "mean" or "std" in isolation
extracteddata<-select(mergeddata,
subjectID,
activityNum,
contains("mean\\(\\)"),
contains("std\\(\\)"))
# obtain the names of each activity for each label
activity_names<-fread(alldata[grep("activity.labels", alldata)])
setnames(activity_names, c("V1", "V2"), c("activityNum", "activity"))
# remove capitalization and underscores
activity_names$activity<-tolower(activity_names$activity)
activity_names$activity<-gsub(pattern = "_",
replacement = "",
x = activity_names$activity)
# merge the data set containing the activity names with the previously
# computated data set
cleandata<-merge(y = extracteddata,
x = activity_names,
by = "activityNum")
cleandata<-arrange(cleandata, subjectID, activityNum)
## create a second, independent tidy data set with the average of each variable for each activity
## and each subject.
# melt the data frame computed in the last step into a long format data frame
meltdata<-melt(cleandata, id=c("activity", "subjectID"))
# cast the long format data to wide format, and call mean() on all variables but subjectID and activity
averaged_data<-dcast(data = meltdata,
formula = subjectID+activity~variable,
fun.aggregate = mean)
#arrange the data by both subjectID and activityNum
averaged_data<-arrange(averaged_data, subjectID, activityNum)
# the column names of averaged_data need to be changed to reflect the fact that the data they contain are averaged values
new_colNames<-paste("ave", colnames(averaged_data), sep=".")
setnames(x = averaged_data,
old = colnames(averaged_data[4:ncol(averaged_data)]),
new = new_colNames[4:length(new_colNames)])
write.table(x = averaged_data,
file = "./Fitness/averagedData.txt",
row.names = F)
data<-read.table(file = "./Fitness/averagedData.txt", header = T)
View(data)
data<-read.table(file = "./Fitness/averagedData.txt", header = T)
View(data)
names(cleandata)
View(activity_names)
View(activity_test)
names(cleandata)
grep(pattern = "BodyAcc", x = names(cleandata))
bodyAccNums<-grep(pattern = "BodyAcc", x = names(cleandata))
names(cleandata)[bodyAccNums]
